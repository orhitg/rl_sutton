{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Arm Bandit problem\n",
    "\n",
    "The name comes from imagining a gambler at a row of slot machines (sometimes known as \"one-armed bandits\"), who has to decide which machines to play, how many times to play each machine and in which order to play them, and whether to continue with the current machine or try a different machine. The multi-armed bandit problem also falls into the broad category of stochastic scheduling.\n",
    "\n",
    "In the problem, each machine provides a random reward from a probability distribution specific to that machine. The objective of the gambler is to maximize the sum of rewards earned through a sequence of lever pulls. The crucial tradeoff the gambler faces at each trial is between \"exploitation\" of the machine that has the highest expected payoff and \"exploration\" to get more information about the expected payoffs of the other machines. The trade-off between exploration and exploitation is also faced in machine learning.\n",
    "\n",
    "## Exploitation\n",
    "When greedy choice is made to select the arm i.e. choose the machine which gives highest immediate award. It is right thing to maximize the expected reward in one step.\n",
    "\n",
    "## Exploration\n",
    "Instead of chosing highest immediate award chose other non-optimal option. It increases the average of non-optimal options. This may produce the greater total expected reward in long run.\n",
    "\n",
    "Here we'll see the difference between both experimentally as explained in book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "K = 10                               # Number of Machines/Arms\n",
    "Q = []                               # Average Reward for Arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Q values for K arms\n",
    "def genQ():\n",
    "    global Q\n",
    "    Q = np.random.normal(0,1,K)\n",
    "\n",
    "# Generate Reward for K arms\n",
    "def genR():\n",
    "    return [np.random.normal(Q[k],1) for k in range(K)]\n",
    "\n",
    "# K arm Bandit algorithm running for N times\n",
    "def kArmBandit(K, N):\n",
    "    genQ()\n",
    "    for n in range(N):\n",
    "        R = genR()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
